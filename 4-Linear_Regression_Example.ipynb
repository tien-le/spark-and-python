{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Example\n",
    "\n",
    "Let's walk through the steps of the official documentation example. Doing this will help your ability to read from the documentation, understand it, and then apply it to your own problems (the upcoming Consulting Project).\n",
    "\n",
    "## Sample Data\n",
    "\n",
    "```\n",
    "-9.490009878824548 1:0.4551273600657362 2:0.36644694351969087 3:-0.38256108933468047 4:-0.4458430198517267 5:0.33109790358914726 6:0.8067445293443565 7:-0.2624341731773887 8:-0.44850386111659524 9:-0.07269284838169332 10:0.5658035575800715\n",
    "0.2577820163584905 1:0.8386555657374337 2:-0.1270180511534269 3:0.499812362510895 4:-0.22686625128130267 5:-0.6452430441812433 6:0.18869982177936828 7:-0.5804648622673358 8:0.651931743775642 9:-0.6555641246242951 10:0.17485476357259122\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:46.380294Z",
     "start_time": "2020-02-11T06:45:43.657297Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"LinearRegression\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:46.441790Z",
     "start_time": "2020-02-11T06:45:46.382633Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:49.156520Z",
     "start_time": "2020-02-11T06:45:46.444232Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load training data\n",
    "dir_input_data_path = \"./input_data/\"\n",
    "file_input_path = dir_input_data_path + \"sample_linear_regression_data.txt\"\n",
    "\n",
    "df = spark.read.format(\"libsvm\").load(file_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:49.437007Z",
     "start_time": "2020-02-11T06:45:49.158317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|             label|            features|\n",
      "+------------------+--------------------+\n",
      "|-9.490009878824548|(10,[0,1,2,3,4,5,...|\n",
      "|0.2577820163584905|(10,[0,1,2,3,4,5,...|\n",
      "+------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:49.607786Z",
     "start_time": "2020-02-11T06:45:49.439455Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=-9.490009878824548, features=SparseVector(10, {0: 0.4551, 1: 0.3664, 2: -0.3826, 3: -0.4458, 4: 0.3311, 5: 0.8067, 6: -0.2624, 7: -0.4485, 8: -0.0727, 9: 0.5658}))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:49.622855Z",
     "start_time": "2020-02-11T06:45:49.609665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! We haven't seen libsvm formats before. In fact the aren't very popular when working with datasets in Python, but the Spark Documentation makes use of them a lot because of their formatting. Let's see what the training data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:49.722245Z",
     "start_time": "2020-02-11T06:45:49.624290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|             label|            features|\n",
      "+------------------+--------------------+\n",
      "|-9.490009878824548|(10,[0,1,2,3,4,5,...|\n",
      "|0.2577820163584905|(10,[0,1,2,3,4,5,...|\n",
      "|-4.438869807456516|(10,[0,1,2,3,4,5,...|\n",
      "+------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the format that Spark expects. Two columns with the names \"label\" and \"features\". \n",
    "\n",
    "The \"label\" column then needs to have the numerical label, either a regression numerical value, or a numerical value that matches to a classification grouping. Later on we will talk about unsupervised learning algorithms that by their nature do not use or require a label.\n",
    "\n",
    "The feature column has inside of it a vector of all the features that belong to that row. Usually what we end up doing is combining the various feature columns we have into a single 'features' column using the data transformations we've learned about.\n",
    "\n",
    "Let's continue working through this simple example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:49.729326Z",
     "start_time": "2020-02-11T06:45:49.724735Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label', 'features']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:49.776280Z",
     "start_time": "2020-02-11T06:45:49.731248Z"
    }
   },
   "outputs": [],
   "source": [
    "# These are the default values for the featuresCol, labelCol, predictionCol\n",
    "\n",
    "# You could also pass in additional parameters for regularization, do the reading \n",
    "# in ISLR to fully understand that, after that its just some simple parameter calls.\n",
    "# Check the documentation with Shift+Tab for more info!\n",
    "lr = LinearRegression(featuresCol=\"features\", \n",
    "                      labelCol=\"label\", \n",
    "                      predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:50.806760Z",
     "start_time": "2020-02-11T06:45:49.778757Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "lr_model = lr.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:50.822317Z",
     "start_time": "2020-02-11T06:45:50.808549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0073350710225801715,0.8313757584337543,-0.8095307954684084,2.441191686884721,0.5191713795290003,1.1534591903547016,-0.2989124112808717,-0.5128514186201779,-0.619712827067017,0.6956151804322931]\n",
      "\n",
      "\n",
      "Intercept:0.14228558260358093\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: {}\".format(str(lr_model.coefficients))) # For each feature...\n",
    "print('\\n')\n",
    "print(\"Intercept:{}\".format(str(lr_model.intercept)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:50.835208Z",
     "start_time": "2020-02-11T06:45:50.824502Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0073, 0.8314, -0.8095, 2.4412, 0.5192, 1.1535, -0.2989, -0.5129, -0.6197, 0.6956])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:50.848395Z",
     "start_time": "2020-02-11T06:45:50.837677Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14228558260358093"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:50.864633Z",
     "start_time": "2020-02-11T06:45:50.849774Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Param(parent='LinearRegression_f83e6de75176', name='loss', doc='The loss function to be optimized. Supported options: squaredError, huber. (Default squaredError)')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:50.882245Z",
     "start_time": "2020-02-11T06:45:50.866409Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a summary attribute that contains even more info!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:50.898050Z",
     "start_time": "2020-02-11T06:45:50.884445Z"
    }
   },
   "outputs": [],
   "source": [
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lr_model.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of info, here are a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:51.105721Z",
     "start_time": "2020-02-11T06:45:50.900297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "|-11.011130022096554|\n",
      "| 0.9236590911176538|\n",
      "|-4.5957401897776675|\n",
      "|  -20.4201774575836|\n",
      "|-10.339160314788181|\n",
      "|-5.9552091439610555|\n",
      "|-10.726906349283922|\n",
      "|  2.122807193191233|\n",
      "|  4.077122222293811|\n",
      "|-17.316168071241652|\n",
      "| -4.593044343959059|\n",
      "|  6.380476690746936|\n",
      "| 11.320566035059846|\n",
      "|-20.721971774534094|\n",
      "| -2.736692773777401|\n",
      "| -16.66886934252847|\n",
      "|  8.242186378876315|\n",
      "|-1.3723486332690233|\n",
      "|-0.7060332131264666|\n",
      "|-1.1591135969994064|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "RMSE: 10.16309157133015\n",
      "r2: 0.027839179518600154\n"
     ]
    }
   ],
   "source": [
    "trainingSummary.residuals.show()\n",
    "print(\"RMSE: {}\".format(trainingSummary.rootMeanSquaredError))\n",
    "print(\"r2: {}\".format(trainingSummary.r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:51.114912Z",
     "start_time": "2020-02-11T06:45:51.108453Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_model_summary = lr_model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:51.136299Z",
     "start_time": "2020-02-11T06:45:51.116973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8068799897284031, 0.817504726374364, 0.8095690515350549, 0.8435177715251213, 0.8066923009911938, 0.8238680228428261, 0.8041910472918519, 0.8095101717564966, 0.8428997032677101, 0.788760166505627, 0.46405794834415603]\n"
     ]
    }
   ],
   "source": [
    "print(lr_model_summary.coefficientStandardErrors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:51.146805Z",
     "start_time": "2020-02-11T06:45:51.138604Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.145215527783876"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model_summary.meanAbsoluteError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:51.158358Z",
     "start_time": "2020-02-11T06:45:51.148343Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103.28843028724194"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model_summary.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:51.175976Z",
     "start_time": "2020-02-11T06:45:51.160879Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.16309157133015"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model_summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Splits\n",
    "\n",
    "But wait! We've commited a big mistake, we never separated our data set into a training and test set. Instead we trained on ALL of the data, something we generally want to avoid doing. Read ISLR and check out the theory lecture for more info on this, but remember we won't get a fair evaluation of our model by judging how well it does again on the same data it was trained on!\n",
    "\n",
    "Luckily Spark DataFrames have an almost too convienent method of splitting the data! Let's see it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:51.186624Z",
     "start_time": "2020-02-11T06:45:51.181013Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load all corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:51.443455Z",
     "start_time": "2020-02-11T06:45:51.189529Z"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"libsvm\").load(file_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:51.551427Z",
     "start_time": "2020-02-11T06:45:51.445227Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=-9.490009878824548, features=SparseVector(10, {0: 0.4551, 1: 0.3664, 2: -0.3826, 3: -0.4458, 4: 0.3311, 5: 0.8067, 6: -0.2624, 7: -0.4485, 8: -0.0727, 9: 0.5658})),\n",
       " Row(label=0.2577820163584905, features=SparseVector(10, {0: 0.8387, 1: -0.127, 2: 0.4998, 3: -0.2269, 4: -0.6452, 5: 0.1887, 6: -0.5805, 7: 0.6519, 8: -0.6556, 9: 0.1749}))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:51.556051Z",
     "start_time": "2020-02-11T06:45:51.552884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:51.850680Z",
     "start_time": "2020-02-11T06:45:51.558764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|              label|\n",
      "+-------+-------------------+\n",
      "|  count|                501|\n",
      "|   mean|0.25688882219498976|\n",
      "| stddev| 10.317884030544564|\n",
      "|    min|-28.571478869743427|\n",
      "|    max|  27.78383192005107|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:51.854453Z",
     "start_time": "2020-02-11T06:45:51.852296Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pass in the split between training/test as a list.\n",
    "# No correct, but generally 70/30 or 60/40 splits are used. \n",
    "# Depending on how much data you have and how unbalanced it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:51.883572Z",
     "start_time": "2020-02-11T06:45:51.855928Z"
    }
   },
   "outputs": [],
   "source": [
    "train_set, test_set = df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:51.888833Z",
     "start_time": "2020-02-11T06:45:51.885739Z"
    }
   },
   "outputs": [],
   "source": [
    "# Review training set and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:52.205501Z",
     "start_time": "2020-02-11T06:45:51.890954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|              label|\n",
      "+-------+-------------------+\n",
      "|  count|                404|\n",
      "|   mean|0.27941508411735216|\n",
      "| stddev| 10.492148919662801|\n",
      "|    min|-28.571478869743427|\n",
      "|    max|  27.78383192005107|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_set.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:52.392116Z",
     "start_time": "2020-02-11T06:45:52.207643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|              label|\n",
      "+-------+-------------------+\n",
      "|  count|                 97|\n",
      "|   mean|0.16306810243586653|\n",
      "| stddev|  9.609010234318017|\n",
      "|    min|-19.782762789614537|\n",
      "|    max| 26.903524792043335|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_set.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we only train on the train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:52.401468Z",
     "start_time": "2020-02-11T06:45:52.394422Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label', 'features']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Fit Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:53.098037Z",
     "start_time": "2020-02-11T06:45:52.403244Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Create Model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Fit Model\n",
    "lr_model = lr.fit(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can directly get a .summary object using the evaluate method and Show the RMSE score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:53.103068Z",
     "start_time": "2020-02-11T06:45:53.099691Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_model_summary = lr_model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:53.349979Z",
     "start_time": "2020-02-11T06:45:53.104861Z"
    }
   },
   "outputs": [],
   "source": [
    "prediction_result = lr_model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Residuals of Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:53.492004Z",
     "start_time": "2020-02-11T06:45:53.351809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "| -20.77398363324071|\n",
      "|-17.232412233213406|\n",
      "| -15.94614835086861|\n",
      "|-15.579687029130524|\n",
      "|-16.120564632791236|\n",
      "|-17.234413337015194|\n",
      "|-13.461913671444306|\n",
      "|-12.393319399015537|\n",
      "| -13.76524027667394|\n",
      "|-10.703912152503811|\n",
      "| -7.704014094082844|\n",
      "| -10.40938185408925|\n",
      "|  -8.49544563110033|\n",
      "| -8.942255084713626|\n",
      "| -9.973151856288984|\n",
      "|-11.013166825397336|\n",
      "| -5.632369222707707|\n",
      "| -9.895470377407369|\n",
      "| -9.208015640940916|\n",
      "| -9.590107983250439|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_result.residuals.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the RMSE of Training set and Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:53.497728Z",
     "start_time": "2020-02-11T06:45:53.493637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of training set : 10.365455688124422\n",
      "RMSE of testing set: 9.359380490785071\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE of training set : {}\".format(lr_model_summary.rootMeanSquaredError))\n",
    "print(\"RMSE of testing set: {}\".format(prediction_result.rootMeanSquaredError))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Prediction Values\n",
    "\n",
    "Well that is nice, but realistically we will eventually want to test this model against unlabeled data, after all, that is the whole point of building the model in the first place. We can again do this with a convenient method call, in this case, transform(). Which was actually being called within the evaluate() method. Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:53.526714Z",
     "start_time": "2020-02-11T06:45:53.499191Z"
    }
   },
   "outputs": [],
   "source": [
    "unlabeled_set = test_set.select(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:53.649545Z",
     "start_time": "2020-02-11T06:45:53.529137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unlabeled_set.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:53.746979Z",
     "start_time": "2020-02-11T06:45:53.651452Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=SparseVector(10, {0: -0.0389, 1: -0.4167, 2: 0.8997, 3: 0.641, 4: 0.2733, 5: -0.2618, 6: -0.2795, 7: -0.1307, 8: -0.0854, 9: -0.0546})),\n",
       " Row(features=SparseVector(10, {0: 0.5176, 1: 0.2305, 2: 0.6163, 3: 0.1691, 4: 0.9695, 5: -0.3471, 6: 0.8527, 7: 0.9896, 8: 0.8806, 9: -0.4345}))]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_set.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:53.763349Z",
     "start_time": "2020-02-11T06:45:53.748248Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_result = lr_model.transform(unlabeled_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T06:45:53.905891Z",
     "start_time": "2020-02-11T06:45:53.765799Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|            features|         prediction|\n",
      "+--------------------+-------------------+\n",
      "|(10,[0,1,2,3,4,5,...| 0.9912208436261734|\n",
      "|(10,[0,1,2,3,4,5,...| -0.571213955451109|\n",
      "|(10,[0,1,2,3,4,5,...|-1.0803439133409376|\n",
      "|(10,[0,1,2,3,4,5,...|-0.5059720118909656|\n",
      "|(10,[0,1,2,3,4,5,...|0.25855530522067516|\n",
      "|(10,[0,1,2,3,4,5,...| 1.5023250647759474|\n",
      "|(10,[0,1,2,3,4,5,...|-0.4051742237144631|\n",
      "|(10,[0,1,2,3,4,5,...|-1.0272753768752207|\n",
      "|(10,[0,1,2,3,4,5,...|   1.20666448781775|\n",
      "|(10,[0,1,2,3,4,5,...|-1.1534382129256144|\n",
      "|(10,[0,1,2,3,4,5,...| -3.936535583805983|\n",
      "|(10,[0,1,2,3,4,5,...|0.11371591805933007|\n",
      "|(10,[0,1,2,3,4,5,...| -1.737993955852824|\n",
      "|(10,[0,1,2,3,4,5,...|-0.9499008431125953|\n",
      "|(10,[0,1,2,3,4,5,...|0.20265130994342195|\n",
      "|(10,[0,1,2,3,4,5,...|  1.523156946572788|\n",
      "|(10,[0,1,2,3,4,5,...|-3.7966187088062338|\n",
      "|(10,[0,1,2,3,4,5,...| 0.6844038063251215|\n",
      "|(10,[0,1,2,3,4,5,...|0.21595999999264612|\n",
      "|(10,[0,1,2,3,4,5,...| 0.9843944684883473|\n",
      "+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so this data is a bit meaningless, so let's explore this same process with some data that actually makes a little more intuitive sense!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
